{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ICR - Identifying Age-Related Conditions Dataset with TensorFlow Decision Forests","metadata":{}},{"cell_type":"markdown","source":"This notebook walks you through how to train a baseline Random Forest model using TensorFlow Decision Forests on the ICR - Identifying Age-Related Conditions dataset made available for this competition. The goal of the model is to predict if a person has one or more of any of three medical conditions or none.\n\nRoughly, the code will look as follows:\n\n```\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\n\ndataset = pd.read_csv(\"project/dataset.csv\")\ntf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n\nmodel = tfdf.keras.RandomForestModel()\nmodel.fit(tf_dataset)\n\nprint(model.summary())\n```\n\nDecision Forests are a family of tree-based models including Random Forests and Gradient Boosted Trees. They are the best place to start when working with tabular data, and will often outperform (or provide a strong baseline) before you begin experimenting with neural networks.","metadata":{}},{"cell_type":"markdown","source":"# Import the libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_decision_forests as tfdf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:32.831186Z","iopub.execute_input":"2023-06-05T09:40:32.831751Z","iopub.status.idle":"2023-06-05T09:40:32.838068Z","shell.execute_reply.started":"2023-06-05T09:40:32.831698Z","shell.execute_reply":"2023-06-05T09:40:32.836913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow v\" + tf.__version__)\nprint(\"TensorFlow Decision Forests v\" + tfdf.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:32.840599Z","iopub.execute_input":"2023-06-05T09:40:32.841063Z","iopub.status.idle":"2023-06-05T09:40:32.857003Z","shell.execute_reply.started":"2023-06-05T09:40:32.84102Z","shell.execute_reply":"2023-06-05T09:40:32.855717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:32.858536Z","iopub.execute_input":"2023-06-05T09:40:32.859576Z","iopub.status.idle":"2023-06-05T09:40:32.885445Z","shell.execute_reply.started":"2023-06-05T09:40:32.859534Z","shell.execute_reply":"2023-06-05T09:40:32.884292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is composed of 58 columns and 617 entries. We can see all 58 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:","metadata":{}},{"cell_type":"code","source":"dataset_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:32.888408Z","iopub.execute_input":"2023-06-05T09:40:32.889143Z","iopub.status.idle":"2023-06-05T09:40:32.917468Z","shell.execute_reply.started":"2023-06-05T09:40:32.889103Z","shell.execute_reply":"2023-06-05T09:40:32.916102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Class` is the label column indicating if a person has one or more of any of the three medical conditions (i.e,`Class 1`), or none of the three medical conditions (i.e,`Class 0`).\nGiven the features of the dataset, the goal of our model is to predict the value of `Class` for any person.","metadata":{}},{"cell_type":"markdown","source":"# Quick basic dataset exploration","metadata":{}},{"cell_type":"code","source":"dataset_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:32.918945Z","iopub.execute_input":"2023-06-05T09:40:32.919274Z","iopub.status.idle":"2023-06-05T09:40:33.089787Z","shell.execute_reply.started":"2023-06-05T09:40:32.919247Z","shell.execute_reply":"2023-06-05T09:40:33.088642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"dataset_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T08:39:29.187353Z","iopub.execute_input":"2023-06-05T08:39:29.187811Z","iopub.status.idle":"2023-06-05T08:39:29.214705Z","shell.execute_reply.started":"2023-06-05T08:39:29.187771Z","shell.execute_reply":"2023-06-05T08:39:29.213636Z"}}},{"cell_type":"markdown","source":"## Pie chart for label column: Class","metadata":{}},{"cell_type":"code","source":"plot_df = dataset_df.Class.value_counts()\nplot_df.plot(kind=\"pie\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:33.091603Z","iopub.execute_input":"2023-06-05T09:40:33.091986Z","iopub.status.idle":"2023-06-05T09:40:33.24413Z","shell.execute_reply.started":"2023-06-05T09:40:33.091951Z","shell.execute_reply":"2023-06-05T09:40:33.242331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Important**: From the pie chart we can see that the dataset is heavily imbalanced since the fraction of positive(`1`) samples is very small compared to the negative(`0`) samples.","metadata":{}},{"cell_type":"markdown","source":"# Numerical data distribution\n\nFirst, we will list all the numerical columns names.","metadata":{}},{"cell_type":"code","source":"# Store all the numerical column names into a list\nNUM_FEATURE_COLUMNS = [i for i in dataset_df.columns if i not in [\"Id\", \"EJ\", \"Class\"]]","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:33.251622Z","iopub.execute_input":"2023-06-05T09:40:33.252301Z","iopub.status.idle":"2023-06-05T09:40:33.260876Z","shell.execute_reply.started":"2023-06-05T09:40:33.252246Z","shell.execute_reply":"2023-06-05T09:40:33.259061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now plot the first 6 numerical columns and their values using bar charts.","metadata":{}},{"cell_type":"code","source":"figure, axis = plt.subplots(3, 2, figsize=(15, 15))\nplt.subplots_adjust(hspace=0.25, wspace=0.3)\n\nfor i, column_name in enumerate(NUM_FEATURE_COLUMNS[:6]):\n    row = i//2\n    col = i % 2\n    bp = sns.barplot(ax=axis[row, col], x=dataset_df['Id'], y=dataset_df[column_name])\n    bp.set(xticklabels=[])\n    #bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 7)\n    axis[row, col].set_title(column_name)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:40:33.263556Z","iopub.execute_input":"2023-06-05T09:40:33.264688Z","iopub.status.idle":"2023-06-05T09:41:03.033466Z","shell.execute_reply.started":"2023-06-05T09:40:33.264629Z","shell.execute_reply":"2023-06-05T09:41:03.03226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also create a list of feature columns that will be used for training. We will drop `Id` from the list since it is not needed.","metadata":{}},{"cell_type":"code","source":"FEATURE_COLUMNS = [i for i in dataset_df.columns if i not in [\"Id\"]]","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.035046Z","iopub.execute_input":"2023-06-05T09:41:03.03565Z","iopub.status.idle":"2023-06-05T09:41:03.040934Z","shell.execute_reply.started":"2023-06-05T09:41:03.035618Z","shell.execute_reply":"2023-06-05T09:41:03.039756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us split the dataset into training and testing datasets:","metadata":{}},{"cell_type":"markdown","source":"# KFold validation\n\nWe will use KFold cross validation for training this model since the normal train/test split training won't be enough to acheive decent score.\n\nWe will split the dataset into 5 consecutive folds. Each fold is then used once as a validation set while the 4 (5-1) remaining folds form the training set.\n\nRead more about KFold [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).","metadata":{}},{"cell_type":"code","source":"# Creates a GroupKFold with 5 splits\nkf = KFold(n_splits=5)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.045968Z","iopub.execute_input":"2023-06-05T09:41:03.046524Z","iopub.status.idle":"2023-06-05T09:41:03.055073Z","shell.execute_reply.started":"2023-06-05T09:41:03.046478Z","shell.execute_reply":"2023-06-05T09:41:03.05377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Out of Fold (OOF)\n\nDuring KFold cross validation, the predictions made on the test set of each fold is known as Out of Fold(OOF) predictions. We will estimate the performance of the model using the predictions made across all the k (5 in this example) folds.\n\nFor our training loop, we will create a pandas dataframe named `oof` to store the predictions of the validation set during each fold.","metadata":{}},{"cell_type":"code","source":"# Create list of ids for the creation of oof dataframe.\nID_LIST = dataset_df.index\n\n# Create a dataframe of required size with zero values.\noof = pd.DataFrame(data=np.zeros((len(ID_LIST),1)), index=ID_LIST)\n\n# Create an empty dictionary to store the models trained for each fold.\nmodels = {}\n\n# Create empty dict to save metircs for the models trained for each fold.\naccuracy = {}\ncross_entropy = {}\n\n# Save the name of the label column to a variable.\nlabel = \"Class\"","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.056498Z","iopub.execute_input":"2023-06-05T09:41:03.057053Z","iopub.status.idle":"2023-06-05T09:41:03.068896Z","shell.execute_reply.started":"2023-06-05T09:41:03.057011Z","shell.execute_reply":"2023-06-05T09:41:03.067335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select a Model\n\nThere are several tree-based models for you to choose from.\n\n* RandomForestModel\n* GradientBoostedTreesModel\n* CartModel\n* DistributedGradientBoostedTreesModel\n\nTo start, we'll work with a Random Forest. This is the most well-known of the Decision Forest training algorithms.\n\nA Random Forest is a collection of decision trees, each trained independently on a random subset of the training dataset (sampled with replacement). The algorithm is unique in that it is robust to overfitting, and easy to use.\n\nWe can list the all the available models in TensorFlow Decision Forests using the following code:","metadata":{}},{"cell_type":"code","source":"tfdf.keras.get_all_models()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.070615Z","iopub.execute_input":"2023-06-05T09:41:03.071887Z","iopub.status.idle":"2023-06-05T09:41:03.08354Z","shell.execute_reply.started":"2023-06-05T09:41:03.071842Z","shell.execute_reply":"2023-06-05T09:41:03.082525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How can I configure them?\n\nTensorFlow Decision Forests provides good defaults for you (e.g. the top ranking hyperparameters on our benchmarks, slightly modified to run in reasonable time). If you would like to configure the learning algorithm, you will find many options you can explore to get a better score.\n\nYou can select a template and/or set parameters as follows:\n\n```rf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\")```\n\nRead more [here](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel).","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning to avoid overfitting\n\nBecause of the smaller size of the dataset, it is likely that the model will overfit during training.\nNumerous parameters, primarily `max_depth` and `num_trees` can be changed to fine-tune the model and prevent overfitting.\n\nThe attribute`max_depth` indicates the maximum depth of the tree. To avoid overfitting, we can try to reduce the depth of the tree from it's default value, which is `16`. Another way to tackle overfitting is to increase the number of individual decision trees. To do this, we have to increase the value of the parameter `num_trees` from its default value(`300`).\n\nYou can set these parameters as follows:\n\n```rf = tfdf.keras.RandomForestModel(max_depth=5, num_trees=500)```","metadata":{}},{"cell_type":"markdown","source":"# Strategies to handle the dataset imbalance\n\nLet's examine the fraction of positive and negative samples in this dataset's  by examining the`Class` column.","metadata":{}},{"cell_type":"code","source":"# Calculate the number of negative and positive values in `Class` column\nneg, pos = np.bincount(dataset_df['Class'])\n# Calculate total samples\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos / total))","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.085138Z","iopub.execute_input":"2023-06-05T09:41:03.085637Z","iopub.status.idle":"2023-06-05T09:41:03.095948Z","shell.execute_reply.started":"2023-06-05T09:41:03.085592Z","shell.execute_reply":"2023-06-05T09:41:03.094995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, positive samples only account for 17.50% of the data. This means that our datastet is heavily imbalanced.\n\nIn classification problems with imbalanced datasets, a model tends to be more biased towards the majority class leading to the model performing poorly on the samples belonging to the minority class.","metadata":{}},{"cell_type":"markdown","source":"There are many techniques for dealing with imbalanced data. The most suitable techniques for this dataset are:\n\n* Undersampling\n* Class weighting\n\nIn this notebook we will use **Class weighting** to deal with imbalanced data. But first, we will quickly outline how undersampling can be performed.","metadata":{}},{"cell_type":"markdown","source":"# Undersampling\n\nOne approach to deal with an imbalanced dataset would be to under-sample the majority class(es) by choosing a smaller subset of the majority class samples(negative or `0` class in this case) from the dataset rather than picking the entire data.\n\nNote: You have to loop through the dataset and try different random subsets for a better score.\n\nThe code snippet below illustrates how to perform undersampling.\n\n```\n# This function generates undersampled dataset.\ndef random_under_sampler(df):\n    # Calculate the number of samples for each label. \n    neg, pos = np.bincount(df['Class'])\n\n    # Choose the samples with class label `1`.\n    one_df = df.loc[df['Class'] == 1] \n    # Choose the samples with class label `0`.\n    zero_df = df.loc[df['Class'] == 0]\n    # Select `pos` number of negative samples.\n    # This makes sure that we have equal number of samples for each label.\n    zero_df = zero_df.sample(n=pos)\n\n    # Join both label dataframes.\n    undersampled_df = pd.concat([zero_df, one_df])\n\n    # Shuffle the data and return\n    return undersampled_df.sample(frac = 1)\n```","metadata":{}},{"cell_type":"markdown","source":"# Class weighting\n\nSince the postive(`1`) `Class` labels are only a small fraction of the dataset, we would want the classifier to heavily weight those examples. You can do this by passing **Keras weights** for each class through a parameter. This will cause the model to \"pay more attention\" to examples from an under-represented class. Read more about class weights [here](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights).","metadata":{}},{"cell_type":"code","source":"# Calculate the number of samples for each label.\nneg, pos = np.bincount(dataset_df['Class'])\n\n# Calculate total samples.\ntotal = neg + pos\n\n# Calculate the weight for each label.\nweight_for_0 = (1 / neg) * (total / 2.0)\nweight_for_1 = (1 / pos) * (total / 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.097389Z","iopub.execute_input":"2023-06-05T09:41:03.097747Z","iopub.status.idle":"2023-06-05T09:41:03.110549Z","shell.execute_reply.started":"2023-06-05T09:41:03.097719Z","shell.execute_reply":"2023-06-05T09:41:03.109307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train and evaluate the models using class weights, use the dict in model.fit() as an argument as shown below.\n\n```model.fit(x=train_ds, class_weight=class_weight)```","metadata":{}},{"cell_type":"markdown","source":"# Train Random Forest Model\n\nToday, we will use the defaults to create the Random Forest Model. By default the model is set to train for a classification task.\nWe will train a model for each fold and after training we will store the model and metrics. Here, we have chosen `accuracy` and `binary_crossentropy` as the metrics.","metadata":{}},{"cell_type":"code","source":"# Loop through each fold\nfor i, (train_index, valid_index) in enumerate(kf.split(X=dataset_df)):\n        print('##### Fold',i+1)\n\n        # Fetch values corresponding to the index \n        train_df = dataset_df.iloc[train_index]\n        valid_df = dataset_df.iloc[valid_index]\n        valid_ids = valid_df.index.values\n        \n        # Select only feature columns for training.\n        train_df = train_df[FEATURE_COLUMNS]\n        valid_df = valid_df[FEATURE_COLUMNS]\n        \n        # There's one more step required before we can train the model. \n        # We need to convert the datatset from Pandas format (pd.DataFrame)\n        # into TensorFlow Datasets format (tf.data.Dataset).\n        # TensorFlow Datasets is a high performance data loading library \n        # which is helpful when training neural networks with accelerators like GPUs and TPUs.\n        # Note: Some column names contains white spaces at the end of their name, \n        # which is non-comaptible with SavedModels save format. \n        # By default, `pd_dataframe_to_tf_dataset` function will convert \n        # this column names into a compatible format. \n        # So you can safely ignore the warnings related to this.\n        train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=label)\n        valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_df, label=label)\n\n        # Define the model and metrics\n        rf = tfdf.keras.RandomForestModel()\n        rf.compile(metrics=[\"accuracy\", \"binary_crossentropy\"]) \n        \n        # Train the model\n        # We will train the model using a one-liner.\n        # Note: you may see a warning about Autograph. \n        # You can safely ignore this, it will be fixed in the next release.\n        # Previously calculated class weights is used to handle imbalance.\n        rf.fit(x=train_ds, class_weight=class_weight)\n        \n        # Store the model\n        models[f\"fold_{i+1}\"] = rf\n        \n        \n        # Predict OOF value for validation data\n        predict = rf.predict(x=valid_ds)\n        \n        # Store the predictions in oof dataframe\n        oof.loc[valid_ids, 0] = predict.flatten() \n        \n        # Evaluate and store the metrics in respective dicts\n        evaluation = rf.evaluate(x=valid_ds,return_dict=True)\n        accuracy[f\"fold_{i+1}\"] = evaluation[\"accuracy\"]\n        cross_entropy[f\"fold_{i+1}\"]= evaluation[\"binary_crossentropy\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:03.112281Z","iopub.execute_input":"2023-06-05T09:41:03.112678Z","iopub.status.idle":"2023-06-05T09:41:16.687853Z","shell.execute_reply.started":"2023-06-05T09:41:03.112646Z","shell.execute_reply":"2023-06-05T09:41:16.686765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the model\nOne benefit of tree-based models is that we can easily visualize them. The default number of trees used in the Random Forests is 300. We can select any tree for display.\n\nLet us pick one model from the `models` dict and select a tree for display.","metadata":{}},{"cell_type":"code","source":"tfdf.model_plotter.plot_model_in_colab(models['fold_1'], tree_idx=0, max_depth=3)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:16.689507Z","iopub.execute_input":"2023-06-05T09:41:16.689885Z","iopub.status.idle":"2023-06-05T09:41:16.705467Z","shell.execute_reply.started":"2023-06-05T09:41:16.689853Z","shell.execute_reply":"2023-06-05T09:41:16.704089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model on the Out of bag (OOB) data and the validation dataset\n\nBefore training, we have manually seperated 20% of the dataset for validation named as `valid_ds`.\n\nWe can also use Out of bag (OOB) score to validate our RandomForestModel.\nTo train a Random Forest Model, a set of random samples from training set are choosen by the algorithm and the rest of the samples are used to finetune the model. The subset of data that is not chosen is known as Out of bag data (OOB).\nOOB score is computed on the OOB data.\n\nRead more about OOB data [here](https://developers.google.com/machine-learning/decision-forests/out-of-bag).\n\nThe training logs show the `binary_crossentropy` evaluated on the out of bag dataset according to the number of trees in the model. Let us plot this for the models of each fold.\n\nNote: Smaller values are better for this hyperparameter.","metadata":{}},{"cell_type":"code","source":"figure, axis = plt.subplots(3, 2, figsize=(10, 10))\nplt.subplots_adjust(hspace=0.5, wspace=0.3)\n\nfor i, fold_no in enumerate(models.keys()):\n    row = i//2\n    col = i % 2\n    logs = models[fold_no].make_inspector().training_logs()\n    axis[row, col].plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    axis[row, col].set_title(f\"Fold {i+1}\")\n    axis[row, col].set_xlabel('Number of trees')\n    axis[row, col].set_ylabel('Loss (out-of-bag)')\n\naxis[2][1].set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:16.706691Z","iopub.execute_input":"2023-06-05T09:41:16.707032Z","iopub.status.idle":"2023-06-05T09:41:17.787398Z","shell.execute_reply.started":"2023-06-05T09:41:16.707001Z","shell.execute_reply":"2023-06-05T09:41:17.786556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also see some general stats on the OOB dataset:","metadata":{}},{"cell_type":"code","source":"for _model in models:\n    inspector = models[_model].make_inspector()\n    print(_model, inspector.evaluation())","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:17.788491Z","iopub.execute_input":"2023-06-05T09:41:17.789328Z","iopub.status.idle":"2023-06-05T09:41:17.805625Z","shell.execute_reply.started":"2023-06-05T09:41:17.789297Z","shell.execute_reply":"2023-06-05T09:41:17.804531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us check the evaluation metrics for each fold and its average value.","metadata":{}},{"cell_type":"code","source":"average_loss = 0\naverage_acc = 0\n\nfor _model in  models:\n    average_loss += cross_entropy[_model]\n    average_acc += accuracy[_model]\n    print(f\"{_model}: acc: {accuracy[_model]:.4f} loss: {cross_entropy[_model]:.4f}\")\n\nprint(f\"\\nAverage accuracy: {average_acc/5:.4f}  Average loss: {average_loss/5:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:17.807152Z","iopub.execute_input":"2023-06-05T09:41:17.807522Z","iopub.status.idle":"2023-06-05T09:41:17.814104Z","shell.execute_reply.started":"2023-06-05T09:41:17.807491Z","shell.execute_reply":"2023-06-05T09:41:17.812905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variable importances\n\nVariable importances generally indicate how much a feature contributes to the model predictions or quality. There are several ways to identify important features using TensorFlow Decision Forests. Let us pick one model from models dict and inspect it.\n\nLet us list the available `Variable Importances` for Decision Trees:","metadata":{}},{"cell_type":"code","source":"inspector = models['fold_1'].make_inspector()\n\nprint(f\"Available variable importances:\")\nfor importance in inspector.variable_importances().keys():\n  print(\"\\t\", importance)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:17.815781Z","iopub.execute_input":"2023-06-05T09:41:17.816479Z","iopub.status.idle":"2023-06-05T09:41:17.832524Z","shell.execute_reply.started":"2023-06-05T09:41:17.816447Z","shell.execute_reply":"2023-06-05T09:41:17.831211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example, let us display the important features for the Variable Importance `NUM_AS_ROOT`.\n\nThe larger the importance score for `NUM_AS_ROOT`, the more impact it has on the outcome of the model.\n\nBy default, the list is sorted from the most important to the least. From the output you can infer that the feature at the top of the list is used as the root node in most number of trees in the random forest than any other feature.","metadata":{}},{"cell_type":"code","source":"# Each line is: (feature name, (index of the feature), importance score)\ninspector.variable_importances()[\"NUM_AS_ROOT\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:17.834253Z","iopub.execute_input":"2023-06-05T09:41:17.83476Z","iopub.status.idle":"2023-06-05T09:41:17.845244Z","shell.execute_reply.started":"2023-06-05T09:41:17.834727Z","shell.execute_reply":"2023-06-05T09:41:17.843887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\ntest_ds_pd = test_df\ntest_df_columns = test_ds_pd.columns.tolist()\nTEST_FEATURE_COLUMNS = [i for i in FEATURE_COLUMNS \\\n                        if i in test_df_columns and i != \"Class\"]\ntest_ds_pd = test_ds_pd[TEST_FEATURE_COLUMNS]\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd)\npredictions = models['fold_1'].predict(test_ds)\nn_predictions= [[round(abs(i-1), 8), i] for i in predictions.ravel()]\nprint(n_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:17.846909Z","iopub.execute_input":"2023-06-05T09:41:17.847281Z","iopub.status.idle":"2023-06-05T09:41:18.070621Z","shell.execute_reply.started":"2023-06-05T09:41:17.84725Z","shell.execute_reply":"2023-06-05T09:41:18.069519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\")\nsample_submission[['class_0', 'class_1']] = n_predictions\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T09:41:18.071881Z","iopub.execute_input":"2023-06-05T09:41:18.072223Z","iopub.status.idle":"2023-06-05T09:41:18.085014Z","shell.execute_reply.started":"2023-06-05T09:41:18.072195Z","shell.execute_reply":"2023-06-05T09:41:18.083652Z"},"trusted":true},"execution_count":null,"outputs":[]}]}